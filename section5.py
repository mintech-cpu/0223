#!/usr/bin/env python
# coding: utf-8

# # 【第5回】BeautifulSoupの使い方③
# 
# これまでの講義で、BeautifulSoupを使って以下のことができるようになりました。
# 
# - 第3回目の講義 : **find()**を使って該当するタグ(例えば`<h2>`)から**1つだけ要素**を取り出す
# - 第4回目の講義 : **find_all()**を使って該当するタグ(例えば`<h2>`)から**すべての要素**を取り出す
# 
# 今回の講義では、クラスを指定して`find_all()`を使うことで、自分が欲しい情報をピンポイントで取得していきたいと思います。
# 
# また、今回の講義では演習も付けています。演習を通して大事なことをお伝えしていきますので、ぜひ最後まで聴いていただけると嬉しいです！
# 
# 
# *※動画の感想を、僕のTwitterにメンションしてツイートしていただけると嬉しいです（ ;  ; ）！*
# 
# Twitter : [@hayatasuuu](https://twitter.com/hayatasuuu)

# # クラスを指定して情報を取得する
# 
# 今回のレクチャーでもPythonの公式ページを使っていきます。
# 
# 前回は、`find_all()`を使ってすべての`<h2>`タグを取得しましたが、今回はさらに条件を絞ってデータ抽出できるようになりましょう！

# ## 必要なライブラリのインポート
# 
# まずはライブラリのインポートをします。

# In[1]:


from bs4 import BeautifulSoup
import requests


# ## STEP1 : RequestsでHTMLを取得する
# 
# こちらも今までどおりですね。`Requests`を使って、Python公式ページへアクセスしていきます。
# 
# https://www.python.org/
# 

# In[2]:


# 変数urlに、Python公式ページのURLを入れる
url = 'https://www.python.org/'

# URLにアクセスした結果を、変数rに代入する
r = requests.get(url)


# ## STEP2 : 取得したHTMLを解析する
# 
# アクセス結果からテキスト情報を取得して、`BeautifulSoup`でHTMLを解析しましょう。

# In[3]:


# BeautifulSoupでHTMLを解析する
soup = BeautifulSoup(r.text)


# あとは、クラスを指定して情報を取得するだけですね。

# ## クラスを指定して情報を取得する
# 
# 今回は、今まで扱っていた`<h2>`タグではなく、`<span>`タグの情報を取得していきたいと思います。
# 
# Python公式ページには、非常にたくさんの`<span>`タグが使われており、クラス指定の効果を体感するには持ってこいだからです。
# 
# <br>
# 
# というわけで、まずはいつも通りに`find_all()`を使って`<span>`タグを取得してみましょう。

# In[8]:


# spanタグの数を確認する
print(len(soup.find_all('span')))
soup.find_all()はリストになっているので、len()で囲む

# Python公式ページで、すべてのspanタグを確認
soup.find_all('span')


# 結果を確認してみると、全部で74個の`<span>`があることが分かりました。
# 
# これらすべての情報が欲しいという機会はあまりなく、やはり「特定の`<span>`タグの情報が欲しい」と思う機会が多いはずです。
# 
# 今回は、たとえば「`say-no-more`というクラス名が付いているテキストだけ取得する」ということを考えていきましょう。
# 
# <br>
# 
# クラスを指定する方法は非常にカンタンで、`find_all()`の引数に`class_`を追加してあげるだけです。

# In[9]:


# say-no-moreクラスを持つspanタグをすべて取得する
soup.find_all('span', class_='say-no-more')


# 取得結果を見てみると、`say-no-more`クラスを持つ`<span>`タグだけ取得できています。
# 
# このように、該当するクラスを指定してタグを取得するときは、`class_`を使ってあげます。
# 
# *※`class`ではなく`class_`が採用されている理由は、Pythonの予約語で`class`が存在するからです。予約語というのは、Pythonであらかじめ使われている名前です！*
# 
# なので、`BeautifulSoup`ではアンダースコアがついたクラスを使ってあげます。
# 
# <br>
# 
# もしアンダースコアが付かない形でクラスを指定したい場合、以下のように辞書の形で書くことも可能です。

# In[ ]:


# 辞書の形でsay-no-moreをクラスに持つspanタグを取得する
上記の方法で大丈夫


# 正確には引数`attrs`に渡しているので、`soup.find_all('span', attrs={'class': 'say-no-more'})`ですが、これは省略して問題ないです。
# 
# <br>
# 
# さらに、複数のクラスを指定することも可能です。
# 
# 今回は`say-no-more`だけでなく、`message`クラスも一緒に取得していきましょう。
# 
# 複数のクラスを指定するには、リストの形で引数に渡してあげます。

# In[10]:


# messageとsay-no-moreをクラスに持つspanタグを取得する
soup.find_all('span', class_=['say-no-more', ' message'])


# こうですね。
# 
# これで複数のクラスを指定して`<span>`タグの情報を取得できるようになりました。
# 
# <br>
# 
# ここまで習得した内容を使っても、かなりできることが増えましたね！
# 
# とは言っても、やはり知識はアウトプットしないと定着しないので、演習問題をやってみましょう！

# # 演習
# 
# [テックダイアリー(僕のブログ)](https://tech-diary.net)の以下の記事で、すべての見出し(`<h2>`と`<h3>`)を取得して、リスト形式で保存しましょう。
# 
# https://tech-diary.net/python-scraping-books/
# 
# <hr>
# 
# ▼完成イメージ
# ```
# ['【厳選4冊】Webスクレイピング(Python)でおすすめの本【実務OK】',
#  'おすすめ① : Python2年生 スクレイピングのしくみ 体験してわかる！会話でまなべる！',
#  'おすすめ② : Pythonクローリング&スクレイピング[増補改訂版]',
#  'おすすめ③ : Pythonスクレイピングの基本と実践 データサイエンティストのためのWebデータ収集術',
#  'おすすめ④ : PythonによるWebスクレイピング 第2版',
#  '【Webスクレイピング学習】Pythonとセットで学ぶべきこと3つ',
#  '学ぶべきこと① : HTML, CSS',
#  '学ぶべきこと② : JavaScript',
#  '学ぶべきこと③ : Web技術の知識',
#  'まずは無料でPythonとスクレイピングを学ぼう【本より分かりやすい】']
# ```
# 
# <hr>
# 
# *※できれば1回のリクエストで情報取得していただけると嬉しいです（ ;  ; ）*

# In[11]:


url = "https://tech-diary.net/python-scraping-books/"
r = requests.get(url)
soup = BeautifulSoup(r.text)


# In[18]:


soup.find_all(['h2', 'h3'])


# In[20]:


header_2and3 = [tag.text for tag in soup.find_all(['h2', 'h3'])]
header_2and3


# In[30]:


header_2and3 = []
for tag in soup.find_all(['h2', 'h3']):
    header_2and3.append(tag.text)

header_2and3
    


# ## 補足
# 
# 今回の演習で取得したヘッダーの中には、1つ余計なものが含まれています。
# 
# 実際に記事を見ていただけると分かりますが、関連記事の情報まで取得してしまっています。
# 
# <br>
# 
# 関連記事の情報は、「記事の見出し(`<h2>`と`<h3>`)を取得する」が目的だったとき、ノイズになってしまいます。
# 
# これを解決するには、以下の方法があります。
# 
# - 取得する範囲を狭くする
# - 該当する要素を削除する
# 
# これらも合わせて見ていきましょう！

# ### 取得する範囲を狭くする
# 
# 「`Requests`で取得したHTML全体に対して」ではなく、「記事の本文に対して」見出しタグを取得すれば問題を解決できます。
# 
# 今まで`soup.find()`のように、`find()`を一回しか使いませんでしたが、実は**何度も使うことが可能**です。
# 
# <br>
# 
# このことを利用すると、記事の本文に対してすべての見出しタグを取得するには、以下のように書くことができます。

# In[22]:


# 記事本文の部分を指定してから見出しタグを取得する
soup.find('article').find_all(['h2', 'h3'])


# 先ほどまでは、余計な`<h2>`タグが含まれていましたが、今回は記事の本文から見出しを取得しているのでノイズ削除できました。
# 
# このように、どんどん範囲を狭めていって情報抽出する方法は、非常によく使います。
# 
# <br>
# 
# また記事の本文から、他に取得したい部分がある可能性が高いので、以下のようにいったん変数に入れてあげるのも良いですね。
# 
# *※僕自身、複数回使う部分を変数に入れて、その後で細かい部分を取りにいくことが多いです！*
# 
# 
# 

# In[23]:


# 変数bodyに、記事本文を入れておく
body = soup.find('article')

# bodyから見出しをすべて取得する
body.find_all(['h2', 'h3'])


# このように書いておけば、何回も`soup.find('article')`を書かずに済みます。

# ### 該当する要素を削除する
# 
# もうひとつは、該当する要素を削除する方法です。
# 
# この方法では`find()`を使って要素を特定したあとで、`extract()`を使ってあげます。

# In[ ]:


# 削除前の見出し数を確認
2つ紹介したところで「どちらの方が良いのか」ですが、可能なかぎり「取得する範囲を狭くする」ことをオススメします。

# 該当する要素を削除する


# 削除後の見出し数を確認


# 見出しの中身を確認


# `extrace()`を使うことで、ノイズになっていた見出しを削除できていることが分かるかと思います。
# 
# これでも必要としていたデータ抽出ができていますね。

# ### おすすめのデータ抽出方法
# 
# 取得したいデータの抽出方法を2つ紹介してきました。
# 
# - 取得する範囲を狭くする
# - 該当する要素を削除する
# 
# 2つ紹介したところで「どちらの方が良いのか」ですが、可能なかぎり「取得する範囲を狭くする」ことをオススメします。
# 
# その理由は「該当する要素を削除する」だと、破壊的な方法になってしまうからです。
# 
# <br>
# 
# 破壊的な方法というのは、取得してきたデータ(`soup`)の中身が変わってしまうということです。
# 
# `extract()`を一度使ってしまうと`soup`の中から消えてしまうので、逆に「関連記事を取得したいな...」と思ったとき、その要素を取得できなくなります。
# 
# 仮に取得したい思ったときは、`requests.get()`を使って、HTMLの情報を取得し直す必要が出てきてしまいます。
# 
# <br>
# 
# そうすると、何回も相手方のWebサイトにアクセスすることになってしまうので、できれば取得したデータを変更しないで欲しいデータを抽出できる「取得する範囲を狭くする」方法を選びましょう！
# 
# <br>
# 
# というわけで、今回はクラス指定して情報を取得する方法と、さらには絞り込みをかける方法まで学習していきました。
# 
# 次回は今までの集大成を込めて、不動産のホームページから情報を抽出していきたいと思います。
# 
# 次回の内容が理解できると、JavaScriptが使われていないWebページであれば、自由自在に情報を取得できるようになります。
# 
# 一緒に頑張っていきましょう！

# In[ ]:




