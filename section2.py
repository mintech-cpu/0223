#!/usr/bin/env python
# coding: utf-8

# # 【第2回】Requestsライブラリの使い方
# 
# 今回は、Pythonを使ったスクレイピング入門で必要になる、`Requests`ライブラリの使い方を習得していきたいと思います。
# 
# *※動画の感想を、僕のTwitterにメンションしてツイートしていただけると嬉しいです（ ;  ; ）！*
# 
# Twitter : [@hayatasuuu](https://twitter.com/hayatasuuu)
# 
# ## スクレイピングの流れ
# 
# そもそも、Pythonを使ってスクレイピングするには、以下のステップが必要になります。
# 
# 1. RequestsでHTMLを取得する
# 2. 取得したHTMLを解析する(BeautifulSoup)
# 3. 自分が欲しい情報を取得する
# 
# つまり、`Requests`ライブラリを習得しないと、Pythonを使ったスクレイピングが始まらないわけです。
# 
# <br>
# 
# いきなりスクレイピングと言いたいところですが、まずは`Requests`ライブラリの習得からしていきましょう。
# 
# ## Requestsライブラリとは？
# 
# `Requests`は、PythonでHTTP通信をするためのライブラリです。
# 
# [»公式ページ : Requests: 人間のためのHTTP — requests-docs-ja 1.0.4 documentation](https://requests-docs-ja.readthedocs.io/en/latest/)
# 
# <br>
# 
# ただ今回はHTTPとか難しいことを考えず、以下のことができるライブラリだと考えておきましょう。
# 
# - URLにアクセスするときに必要になるライブラリ
# - Webページの情報(HTML)を取得できるライブラリ
# - APIにアクセスできるライブラリ
# 
# 
# *※APIについては、この講義シリーズで紹介していきます！*
# 
# <br>
# 
# まずはこれくらいの認識で十分です！
# 
# またPythonでは、ほとんど同じことができる`urllib`というライブラリがあります。
# 
# <br>
# 
# 本によっては、HTMLを取得する部分で`urllib`が使われていますが、今回は「より簡潔に書きたい」という目的があるので、`Requests`ライブラリを使っていきましょう。
# 
# *※`Requests`は外部の(pip install しないといけない)ライブラリなので、pip installが使えない、もしくは外部ライブラリを使いたくない場合は、`urllib`が良いです。*
# 

# # Pythonを使ってHTML情報を取得する
# 
# さっそくRequestsライブラリを使って、HTML情報を取得してみましょう。
# 
# 今回は[Pythonの公式ページ](https://www.python.org/)を使って、HTMLの情報を取得する練習をしていきたいと思います。

# ## ライブラリのインポート
# 
# Anacondaを使っている場合は、最初からRequestsライブラリが入っているはずです。Python単体を使っている場合には、下記コマンドにてライブラリのインストールをしていきましょう。

# In[ ]:


# ! pip install requests


# Anacondaを使っている場合は、以下のようにライブラリのインポートから開始すれば大丈夫です！

# In[1]:


import requests


# これで準備が完了しました。それでは、Pythonの公式ページへアクセスしてみましょう。

# ## Python公式ページへアクセス
# 
# Pythonの公式ページURLは、以下になっています。
# 
# https://www.python.org/
# 
# <br>
# 
# こちらにアクセスしていくのですが、その方法はとっても簡単で、以下のように書いてあげるだけです。
# 
# *※Pythonを使ってネットワーク通信をしているので、PCがネットに繋がっている必要があります。*
# 
# 

# In[2]:


# 変数urlに、Python公式ページのURLを代入する
url = "https://www.python.org/"

# 変数rにリクエスト結果を代入する
r = requests.get(url)


# In[11]:


url = "https://www.starbucks.co.jp/"
r = requests.get(url)


# ここでやっていることは、以下のとおりです。
# 
# 1. 変数`url`にPython公式ページのURLを文字列で格納してあげる
# 2. `requests.get()`を使って、Python公式ページにアクセス
# 3. 変数`r`にアクセスした結果を保存する
# 
# 変数`r`については、本とか他の教材だと`response`とか`res`が多いと思います。
# 
# 僕は普段からのクセで`r`を使っていますが、分かりにくかったら`res`とかでも大丈夫です。
# 
# <br>
# 
# やっていること自体は、かなりシンプルですよね。しかも分かりやすい！
# 
# あとはプロパティ(`r.〇〇`)で変数`r`から欲しい情報を取得してあげるだけです。
# 
# 

# ### アクセスしたURLを確認
# 
# アクセスしたURLを確認したいときは、`r.url`を使ってあげます。

# In[4]:


# リクエストしたときのURLを確認する
print(r.url)


# In[12]:


print(r.url)


# ちゃんとPythonの公式ページURLが取得できていますね！(*そりゃ自分でアクセスしたんだから当たり前だろ！と言われそうですが...*)

# ### ステータスコードを確認
# 
# たまにホームページを開いたとき、`403`と表示されることはありませんか？
# 
# その`403`とかを**ステータスコード**と言います。
# 
# <br>
# 
# requestsもネットワーク通信しているため、`r.status_code`を使えばステータスコードを確認できます。

# In[5]:


# リクエストしたときのステータスコードを確認する
r.status_code

r.status_code
# 今回は200が表示されました。これは「アクセスに成功した」ということになります。
# 
# 逆にエラーになったときは、400〜499の数字だったり、500〜599の数字が返ってきます。
# 
# この2つの違いは、以下のとおりです。
# 
# - 400-499 : クライアント側のエラー(つまり、アクセスしようとしている人に問題があります)
# - 500-599 : サーバー側のエラー(つまり、アクセス先に問題があります)
# 
# <br>
# 
# ステータスコードは、例えば以下のような使い方をします。

# In[9]:


if r.status_code == 200:
    print('アクセスに成功したときの処理')
else:
    raise
    print()


# これは「アクセスに成功したら`print()`関数を実行して、それ以外だったら`raise`で意図的にエラーを返す」という意味です。
# 
# もちろん`raise`の部分はエラーを発生させなくても、ログを出力するだけでもOKです。

# ### リクエスト結果の取得
# 
# Python公式ページの情報を取得するためには、`r.text`を使ってあげます。

# In[10]:


# リクエスト結果の取得
print(r.text)


# In[13]:


print(r.text)


# そうすると、莫大な量の文字列が出力されるかと思います。
# 
# これは何かというと、**Python公式ページのHTML**です。
# 
# つまり「スクレイピングに必要なデータを、Web上から取得できた」ということになります。
# 
# <br>
# 
# スクレイピングに必要な手順を思い返すと、以下のようになっていました。
# 
# 1. RequestsでHTMLを取得する
# 2. 取得したHTMLを解析する(BeautifulSoup)
# 3. 自分が欲しい情報を取得する
# 
# `r.text`を使うことで、スクレイピングで必要な手順①が完了したということになります。
# 
# <br>
# 
# ただ、現状だとHTMLを取得しただけで、必要な情報を取り出すことが難しいです。
# 
# たとえば、「`Get Started`や`Download`などの文字列を取得したい」と思ったとき、まずは"Pythonで簡単に取り扱える状態にする"必要があります。
# 
# <br>
# 
# そのために行うのがHTMLの解析で、PythonでHTMLを解析するためには`BeautifulSoup`というライブラリが必要になります。
# 
# というわけで、次回は`BeautifulSoup`も組み合わせることで、自分が取得したいデータをゲットしていきましょう！
